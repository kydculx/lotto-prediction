import requests
import json
import time
from pathlib import Path
from typing import List, Dict, Optional
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

class LottoCrawler:
    """ë¡œë˜ ë‹¹ì²¨ë²ˆí˜¸ ê³µì‹ ì›¹ í¬ë¡¤ëŸ¬ (ë™í–‰ë³µê¶Œ) - ë²Œí¬ ìµœì í™” ë²„ì „"""
    
    # ê³µì‹ ì‚¬ì´íŠ¸ AJAX API URL
    API_URL = "https://www.dhlottery.co.kr/lt645/selectPstLt645Info.do"
    REFERER_URL = "https://www.dhlottery.co.kr/lt645/result"
    
    def __init__(self, data_path: str = None):
        if data_path is None:
            project_root = Path(__file__).parent.parent
            data_path = project_root / "data" / "lotto_results.json"
        self.data_path = Path(data_path)
        self.results = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': self.REFERER_URL,
            'X-Requested-With': 'XMLHttpRequest',
            'Accept': 'application/json, text/javascript, */*; q=0.01'
        }
        
    def load_existing_data(self):
        """ê¸°ì¡´ JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.data_path.exists():
            try:
                with open(self.data_path, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                self.results.sort(key=lambda x: x['round'])
                logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.results)}ê°œ íšŒì°¨")
            except Exception as e:
                logger.error(f"âš ï¸ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.results = []

    def _parse_item(self, item: Dict) -> Dict:
        """API ì‘ë‹µ ì•„ì´í…œì„ ê³µí†µ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
        raw_date = item['ltRflYmd']
        formatted_date = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}"
        return {
            "round": int(item['ltEpsd']),
            "date": formatted_date,
            "numbers": sorted([
                int(item['tm1WnNo']),
                int(item['tm2WnNo']),
                int(item['tm3WnNo']),
                int(item['tm4WnNo']),
                int(item['tm5WnNo']),
                int(item['tm6WnNo'])
            ]),
            "bonus": int(item['bnsWnNo'])
        }

    def fetch_all(self, force=False):
        """ëª¨ë“  íšŒì°¨ ë˜ëŠ” ëˆ„ë½ëœ íšŒì°¨ë¥¼ ë²Œí¬ APIë¥¼ í†µí•´ í•œ ë²ˆì— ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        self.load_existing_data()
        
        logger.info("ğŸ“¡ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ì¡°íšŒ ì¤‘ì…ë‹ˆë‹¤ (Bulk Fetch)...")
        try:
            params = {'srchLtEpsd': 'all'}
            response = requests.get(self.API_URL, params=params, headers=self.headers, timeout=20)
            if response.status_code != 200:
                logger.error(f"âŒ API ì—°ê²° ì‹¤íŒ¨ (Status: {response.status_code})")
                return

            all_data = response.json()
            if not all_data.get('data') or not all_data['data'].get('list'):
                logger.error("âŒ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            raw_list = all_data['data']['list']
            # ì „ì²´ ë°ì´í„°ë¥¼ íŒŒì‹±
            web_results = [self._parse_item(item) for item in raw_list]
            web_results.sort(key=lambda x: x['round'])
            
            latest_on_web = web_results[-1]['round'] if web_results else 0
            latest_stored = self.results[-1]['round'] if self.results else 0

            if not force and latest_stored >= latest_on_web:
                logger.info(f"âœ¨ ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. (ë¡œì»¬: {latest_stored}, ì›¹: {latest_on_web})")
                return

            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±° ë° ìµœì‹ í™”)
            stored_rounds = {r['round'] for r in self.results}
            new_count = 0
            for item in web_results:
                if item['round'] not in stored_rounds:
                    self.results.append(item)
                    new_count += 1
            
            if new_count > 0:
                self.results.sort(key=lambda x: x['round'])
                self.save_data()
                logger.info(f"ğŸ‰ ì´ {new_count}ê°œ íšŒì°¨ì˜ ëˆ„ë½ëœ ë°ì´í„°ê°€ ë²Œí¬ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("ğŸ’¤ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def save_data(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        self.data_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.data_path}")

if __name__ == "__main__":
    crawler = LottoCrawler()
    crawler.fetch_all()
