import requests
from bs4 import BeautifulSoup
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Optional

class LottoCrawler:
    """ë¡œë˜ ë‹¹ì²¨ë²ˆí˜¸ ì›¹ í¬ë¡¤ëŸ¬"""
    
    BASE_URL = "https://www.lotto.co.kr/article/list/AC01"
    AJAX_URL = "https://www.lotto.co.kr/lotto_info/list_ajax"
    
    def __init__(self, data_path: str = None):
        if data_path is None:
            project_root = Path(__file__).parent.parent
            data_path = project_root / "data" / "lotto_results.json"
        self.data_path = Path(data_path)
        self.results = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'X-Requested-With': 'XMLHttpRequest',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Referer': self.BASE_URL
        }
    
    def load_existing_data(self):
        """ê¸°ì¡´ JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.data_path.exists():
            try:
                with open(self.data_path, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                # íšŒì°¨ ìˆœìœ¼ë¡œ ì •ë ¬
                self.results.sort(key=lambda x: x['round'])
                print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.results)}ê°œ íšŒì°¨")
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.results = []
    
    def get_latest_round_num(self) -> int:
        """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°€ì¥ ìµœì‹  íšŒì°¨ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # ì²« í˜ì´ì§€ AJAX ìš”ì²­ìœ¼ë¡œ ìµœì‹  íšŒì°¨ í™•ì¸
            payload = "category=AC01&startPos=0&endPos=10&pageSize=10&page=1"
            response = requests.post(self.AJAX_URL, data=payload, headers=self.headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                first_round_span = soup.select_one('.wnr_cur_list li span')
                if first_round_span:
                    import re
                    match = re.search(r'(\d+)íšŒ', first_round_span.get_text())
                    if match:
                        return int(match.group(1))
        except Exception as e:
            print(f"âŒ ìµœì‹  íšŒì°¨ í™•ì¸ ì‹¤íŒ¨: {e}")
        return 0

    def parse_html_fragment(self, html: str) -> List[Dict]:
        """AJAX ì‘ë‹µ HTMLì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html, 'html.parser')
        new_data = []
        
        # <ul> ë‚´ì˜ <li> í•­ëª©ë“¤ íŒŒì‹±
        items = soup.select('.wnr_cur_list li')
        
        for item in items:
            try:
                spans = item.select('span')
                if not spans: continue
                
                # íšŒì°¨ ë²ˆí˜¸ (ì²« ë²ˆì§¸ span)
                round_text = spans[0].get_text()
                import re
                round_match = re.search(r'(\d+)íšŒ', round_text)
                if not round_match: continue
                round_num = int(round_match.group(1))
                
                # ë‚ ì§œ (ë‘ ë²ˆì§¸ span)
                draw_date = spans[1].get_text() if len(spans) > 1 else ""
                
                # ë‹¹ì²¨ ë²ˆí˜¸ ì¶”ì¶œ (img íƒœê·¸ì˜ src ë˜ëŠ” alt)
                # ì‹¤ì œ ë²ˆí˜¸ëŠ” srcì˜ íŒŒì¼ëª…ì— ìˆìŒ (ì˜ˆ: /.../6.png -> 6)
                imgs = item.select('.cur_wnr_item img')
                numbers = []
                bonus = None
                
                # ì´ë¯¸ì§€ src íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œ
                for img in imgs:
                    src = img.get('src', '')
                    import os
                    filename = os.path.basename(src)
                    # íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: 6.png -> 6)
                    num_match = re.search(r'(\d+)', filename)
                    
                    if num_match:
                        num = int(num_match.group(1))
                        # ì´ë¯¸ì§€ëŠ” ë³´í†µ 7ê°œê°€ ë‚˜ì˜´ (6ê°œ ë‹¹ì²¨ + 1ê°œ ë³´ë„ˆìŠ¤)
                        # 'servic' ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ì´ë¯¸ì§€ëŠ” ì œì™¸ ì²˜ë¦¬ í•„ìš” (í•„í„°ë§)
                        if 'lottoball' in src:
                            if len(numbers) < 6:
                                numbers.append(num)
                            else:
                                bonus = num
                
                if len(numbers) == 6:
                    new_data.append({
                        "round": round_num,
                        "date": draw_date,
                        "numbers": sorted(numbers),
                        "bonus": bonus
                    })
            except Exception as e:
                print(f"âš ï¸ íŒŒì‹± ì¤‘ í•­ëª© ê±´ë„ˆëœ€ (íšŒì°¨ ë¯¸ìƒ): {e}")
                
        return new_data

    def fetch_all(self, force=False):
        """ì „ì²´ íšŒì°¨ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì‹ ê·œ íšŒì°¨ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        self.load_existing_data()
        latest_on_web = self.get_latest_round_num()
        latest_stored = self.results[-1]['round'] if self.results else 0
        
        if not force and latest_stored >= latest_on_web:
            print(f"âœ¨ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. (ë¡œì»¬: {latest_stored}, ì›¹: {latest_on_web})")
            return
        
        print(f"ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘... (ì›¹ ìµœì‹ : {latest_on_web}íšŒ)")
        
        all_new_results = []
        page = 1
        page_size = 10
        total_collected = 0
        
        while True:
            print(f"ğŸ“¥ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
            start_pos = (page - 1) * page_size
            end_pos = page * page_size
            
            payload = f"category=AC01&startPos={start_pos}&endPos={end_pos}&pageSize={page_size}&total={latest_on_web}&page={page}"
            
            try:
                response = requests.post(self.AJAX_URL, data=payload, headers=self.headers, timeout=10)
                if response.status_code != 200 or not response.text.strip():
                    break
                
                page_data = self.parse_html_fragment(response.text)
                if not page_data:
                    break
                
                # ì¤‘ë³µ í™•ì¸ ë° ì¢…ë£Œ ì¡°ê±´
                stop_crawling = False
                for d in page_data:
                    if not any(r['round'] == d['round'] for r in self.results) and \
                       not any(r['round'] == d['round'] for r in all_new_results):
                        all_new_results.append(d)
                    else:
                        if not force:
                            stop_crawling = True
                            break
                
                if stop_crawling:
                    break
                    
                page += 1
                time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        if all_new_results:
            self.results.extend(all_new_results)
            self.results.sort(key=lambda x: x['round'])
            self.save_data()
            print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! {len(all_new_results)}ê°œ ì‹ ê·œ íšŒì°¨ ì¶”ê°€ë¨.")
        else:
            print("ğŸ’¤ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def save_data(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        self.data_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.data_path}")

if __name__ == "__main__":
    crawler = LottoCrawler()
    # ì²˜ìŒ ì‹¤í–‰ ì‹œì—ëŠ” force=Trueë¡œ ì „ì²´ ìˆ˜ì§‘ ê°€ëŠ¥, ì´í›„ì—ëŠ” Falseë¡œ ì¦ë¶„ ì—…ë°ì´íŠ¸
    crawler.fetch_all(force=False)
