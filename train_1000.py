#!/usr/bin/env python3
"""
ğŸ“ 1~1000íšŒì°¨ ë°ì´í„°ë¡œë§Œ í•™ìŠµ (ê°€ì¤‘ì¹˜ ìµœì í™”)
"""

import sys
import os

# ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ë¶€ ìŠ¤ë ˆë”© ë¹„í™œì„±í™” (í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬í™” ì§‘ì¤‘)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["VECLIB_MAXIMUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"

import json
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from src.data_loader import LottoDataLoader
from src.ensemble_predictor import EnsemblePredictor
from src.optimization_cache import OptimizationCache
import numpy as np

import multiprocessing as mp
from functools import partial
import threading
import time


def worker_eval_cached(args):
    """ìºì‹œëœ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì´ˆê³ ì† í‰ê°€"""
    weights, cached_data = args
    avg_hits, _ = OptimizationCache.evaluate_weights(cached_data, weights)
    return avg_hits, weights


def worker_eval(args):
    """(êµ¬) ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—…ì í•¨ìˆ˜ wrapper - ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨"""
    weights, matrix, test_rounds = args
    avg_hits, _ = run_backtest(matrix, weights, test_rounds=test_rounds, label="parallel")
    return avg_hits, weights


def run_backtest(matrix, weights, test_rounds=100, label=""):
    """ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰"""
    n_draws = len(matrix)
    hit_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
    
    for i in range(test_rounds + 1):
        if not label and i % 10 == 0:
            progress = i / test_rounds
            bar_len = 20
            filled_len = int(bar_len * progress)
            bar = "â–ˆ" * filled_len + "â–‘" * (bar_len - filled_len)
            print(f"\r|{bar}| {i:3d}/{test_rounds:3d}", end="", flush=True)
            
        if i == test_rounds:
            break
            
        test_idx = n_draws - test_rounds + i
        train_matrix = matrix[:test_idx]
        
        if len(train_matrix) < 100:
            continue
        
        # ì˜ˆì¸¡
        predictor = EnsemblePredictor(train_matrix, weights=weights, use_ml=True, use_validator=True)
        predicted, _ = predictor.predict_single_set()
        
        # ì‹¤ì œ ë²ˆí˜¸
        actual = set(matrix[test_idx])
        
        hits = len(set(predicted) & actual)
        hit_counts[hits] += 1
    
    total = sum(hit_counts.values())
    avg_hits = sum(h * c for h, c in hit_counts.items()) / total if total > 0 else 0
    
    return avg_hits, hit_counts


def mutate_weights(weights, mutation_rate=0.1):
    """ê°€ì¤‘ì¹˜ ëŒì—°ë³€ì´"""
    new_weights = weights.copy()
    
    engines = list(new_weights.keys())
    
    for _ in range(2):
        eng1, eng2 = np.random.choice(engines, size=2, replace=False)
        
        delta = np.random.uniform(0.01, mutation_rate)
        
        if new_weights[eng1] > delta:
            new_weights[eng1] -= delta
            new_weights[eng2] += delta
    
    # ì •ê·œí™”
    total = sum(new_weights.values())
    return {k: v / total for k, v in new_weights.items()}


def genetic_optimize(matrix, generations=10, population_size=10, test_rounds=100):
    """ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ìµœì í™”"""
    
    # ì´ˆê¸° ê°€ì¤‘ì¹˜ ì„¤ì •
    result_path = Path(__file__).parent / "trained_weights_1000.json"
    if result_path.exists():
        try:
            with open(result_path, 'r') as f:
                data = json.load(f)
            base_weights = data['weights']
            best_score = data.get('best_score', 0)
            print(f"ğŸ”„ ê¸°ì¡´ í•™ìŠµ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ (ì—­ëŒ€ ìµœê³  ì ìˆ˜: {best_score:.4f})")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ í•™ìŠµ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤: {e}")
            base_weights = {
                'statistical': 0.1600,
                'lstm': 0.1200,
                'sequence_correlation': 0.1200,
                'timeseries': 0.1000,
                'advanced_pattern': 0.1000,
                'pattern': 0.0800,
                'gap': 0.0800,
                'graph': 0.0800,
                'poisson': 0.0800,    # ì‹ ê·œ ë¶„ì„ê¸° ì¶”ê°€
                'fourier': 0.0800,    # ì‹ ê·œ ë¶„ì„ê¸° ì¶”ê°€
            }
            best_score = 0
    else:
        print("ğŸ’¡ ì‹ ê·œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤ (ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©)")
        base_weights = {
            'statistical': 0.1600,
            'ml': 0.1000,
            'lstm': 0.1000,
            'sequence_correlation': 0.1000,
            'timeseries': 0.0900,
            'advanced_pattern': 0.0900,
            'pattern': 0.0800,
            'gap': 0.0800,
            'graph': 0.0800,
            'poisson': 0.0600,
            'fourier': 0.0500,
            'numerology': 0.0100,
        }
        best_score = 0
    
    # ì´ˆê¸° ê°œì²´êµ° ìƒì„±
    population = [base_weights.copy()]
    for _ in range(population_size - 1):
        population.append(mutate_weights(base_weights, 0.15))
    
    best_weights = base_weights.copy()
    
    print("=" * 60)
    print("ğŸ“ 1~1000íšŒì°¨ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"   ì„¸ëŒ€ ìˆ˜: {generations}")
    print(f"   ê°œì²´êµ° í¬ê¸°: {population_size}")
    print(f"   ê²€ì¦ íšŒì°¨: {test_rounds}")
    print()
    
    # âš¡ï¸ ìµœì í™” ìºì‹œ ìƒì„± (ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ë¬´ê±°ìš´ ì—°ì‚° ìˆ˜í–‰)
    cache = OptimizationCache()
    cached_data = cache.precalculate(matrix, test_rounds)
    
    # ê°€ìš© ì½”ì–´ ì „ì²´ ì‚¬ìš© (ì‹œìŠ¤í…œ ì—¬ìœ ë¶„ 1ê°œ í™•ë³´)
    num_cores = max(1, mp.cpu_count() - 1)
    
    for gen in range(generations):
        print(f"\n{'='*60}")
        print(f"ğŸ§¬ ì„¸ëŒ€ {gen+1}/{generations} í‰ê°€ ì¤‘ (CPU ì½”ì–´ {num_cores}ê°œ í™œìš©)")
        print(f"{'='*60}")
        
        # ë³‘ë ¬ í‰ê°€ë¥¼ ìœ„í•œ ì¸ì ì¤€ë¹„ (ìºì‹œ ë°ì´í„° ì „ë‹¬)
        # cached_dataëŠ” ì½ê¸° ì „ìš©ì´ë¯€ë¡œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ê³µìœ  ê°€ëŠ¥
        task_args = [(w, cached_data) for w in population]
        
        # ìƒíƒœ ê³µìœ  ë³€ìˆ˜
        completed_count = 0
        current_best_in_gen = 0.0
        lock = threading.Lock()
        
        def _progress_monitor():
            start_time = time.time()
            spinner = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
            idx = 0
            
            while completed_count < population_size:
                elapsed = time.time() - start_time
                percent = completed_count / population_size * 100
                bar_len = 30
                filled = int(bar_len * completed_count / population_size)
                bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
                
                spin = spinner[idx % len(spinner)]
                idx += 1
                
                # ë©”ì¸ ìŠ¤ë ˆë“œì™€ ê°’ ì¶©ëŒ ë°©ì§€ (ì½ê¸°ë§Œ í•˜ë¯€ë¡œ lock ì—†ì–´ë„ ì•ˆì „í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ)
                curr_score = current_best_in_gen
                
                status = f"\r  {spin} [ì‹œê°„: {elapsed:3.0f}s] |{bar}| {percent:5.1f}% ({completed_count}/{population_size}) - ìµœê³ ì ìˆ˜: {curr_score:.4f}"
                sys.stdout.write(status)
                sys.stdout.flush()
                time.sleep(0.1)
                
            # ì™„ë£Œ í›„ ìµœì¢… ì¶œë ¥
            elapsed = time.time() - start_time
            sys.stdout.write(f"\r  âœ… [ì‹œê°„: {elapsed:3.0f}s] |{'â–ˆ'*30}| 100.0% ({population_size}/{population_size}) - ìµœê³ ì ìˆ˜: {current_best_in_gen:.4f}\n")
            sys.stdout.flush()

        # í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„± ë° ì‹¤í–‰
        pool = mp.Pool(processes=num_cores)
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        monitor_thread = threading.Thread(target=_progress_monitor)
        monitor_thread.start()
        
        try:
            results = []
            # imap_unordered ì‚¬ìš©
            for i, res in enumerate(pool.imap_unordered(worker_eval_cached, task_args)):
                score, weights = res
                results.append((score, weights))
                
                with lock:
                    completed_count += 1
                    if score > current_best_in_gen:
                        current_best_in_gen = score
            
            monitor_thread.join() # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
            pool.close()
            pool.join()
            fitness = results
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ìœ„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...")
            pool.terminate()
            pool.join()
            raise # ìƒìœ„ë¡œ ì „ë‹¬í•˜ì—¬ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            pool.terminate()
            pool.join()
            raise
            
        print() 
        
        # ì •ë ¬
        fitness.sort(key=lambda x: x[0], reverse=True)
        
        generation_best_score = fitness[0][0]
        generation_best_weights = fitness[0][1]
        
        # ìµœê³  ê¸°ë¡ ê°±ì‹ 
        if generation_best_score > best_score:
            best_score = generation_best_score
            best_weights = generation_best_weights
            print(f"\nğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! {best_score:.4f}")
            
            # ğŸ’¾ ì¦‰ì‹œ ìë™ ì €ì¥
            try:
                save_data = {
                    "best_score": best_score,
                    "weights": best_weights,
                    "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                with open(result_path, 'w', encoding='utf-8') as f:
                    json.dump(save_data, f, indent=2, ensure_ascii=False)
                print(f"   ğŸ’¾ ê°€ì¤‘ì¹˜ê°€ ìë™ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_path.name}")
            except Exception as e:
                print(f"   âš ï¸ ìë™ ì €ì¥ ì‹¤íŒ¨: {e}")
        else:
            print(f"\n   í˜„ì¬ ì„¸ëŒ€ ìµœê³ : {fitness[0][0]:.4f} | ì—­ëŒ€ ìµœê³ : {best_score:.4f}")
        
        # ìƒìœ„ 50% ì„ íƒ
        survivors = [w for _, w in fitness[:population_size // 2]]
        
        # ìƒˆ ê°œì²´êµ° ìƒì„±
        new_population = survivors.copy()
        while len(new_population) < population_size:
            parent = survivors[np.random.randint(len(survivors))]
            child = mutate_weights(parent, 0.08)
            new_population.append(child)
        
        population = new_population
    
    print()
    print("=" * 60)
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   ìµœê³  ì ìˆ˜: {best_score:.4f}")
    print("=" * 60)
    
    return best_weights, best_score


def main():
    # ë°ì´í„° ë¡œë“œ
    print("\nâ³ ë°ì´í„° ë¡œë”©...")
    loader = LottoDataLoader()
    full_matrix = loader.get_numbers_matrix()
    
    # 1~1000íšŒì°¨ë§Œ ì‚¬ìš©
    train_matrix = full_matrix[:1000]
    
    print(f"âœ… í•™ìŠµ ë°ì´í„°: 1~1000íšŒì°¨ (ì´ {len(train_matrix)}ê°œ)")
    print(f"ğŸ“Œ 1001íšŒì°¨ ì´í›„ëŠ” ì‹¤ì „ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë³´ì¡´\n")
    
    # ìœ ì „ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”
    best_weights, best_score = genetic_optimize(
        train_matrix,
        generations=10000,   # ë¬´í•œì— ê°€ê¹Œìš´ ë°˜ë³µ (ì‚¬ìš©ìê°€ ì¤‘ë‹¨í•  ë•Œê¹Œì§€)
        population_size=12,
        test_rounds=200
    )
    
    print("\n" + "="*60)
    print("âœ… í•™ìŠµì´ ì¤‘ë‹¨ë˜ê±°ë‚˜ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ìµœì í™”ëœ ê°€ì¤‘ì¹˜:")
    for name, weight in sorted(best_weights.items(), key=lambda x: x[1], reverse=True):
        bar = "â–ˆ" * int(weight * 50)
        print(f"  {name:22s}: {weight:.4f} {bar}")
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'training_rounds': '1-1000',
        'best_score': best_score,
        'weights': best_weights
    }
    
    result_path = Path(__file__).parent / "trained_weights_1000.json"
    with open(result_path, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"\nğŸ“ í•™ìŠµ ê²°ê³¼ê°€ {result_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("\nğŸ’¡ ì´ì œ ì´ ê°€ì¤‘ì¹˜ë¡œ 1001íšŒì°¨ ì´í›„ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
